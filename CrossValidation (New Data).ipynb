{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "import time\n",
    "start_time = time.time()\n",
    "from copy import deepcopy, copy\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import shutil\n",
    "import os\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "# from featext2 import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "#matplotlib qt\n",
    "# inline (suitable for ipython only, shown inside browser!) or qt (suitable in general, shown in external window!)\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D #, axes3d\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, ParameterGrid, KFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.decomposition import PCA, KernelPCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoCV, RandomizedLasso\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest, chi2, f_classif, mutual_info_classif, SelectFdr\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "import datetime\n",
    "import urllib\n",
    "import tarfile\n",
    "# import joblib\n",
    "# from joblib import Parallel, delayed, Memory\n",
    "from tempfile import mkdtemp\n",
    "import copy_reg\n",
    "import types\n",
    "import itertools\n",
    "from collections import Counter\n",
    "#import multiprocessing\n",
    "def _pickle_method(m):\n",
    "    if m.im_self is None:\n",
    "        return getattr, (m.im_class, m.im_func.func_name)\n",
    "    else:\n",
    "        return getattr, (m.im_self, m.im_func.func_name)\n",
    "copy_reg.pickle(types.MethodType, _pickle_method)\n",
    "\n",
    "h = .2  # step size in the mesh\n",
    "names = [\"NearNb\", \"RBFSVM1\", \"NaiveBayes\", \"MLP1\", \"Log.Regr\", \"RandFor\", \"AdaBoost\", \"EnsembleMLP\"]\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    SVC(gamma='auto', C=1),\n",
    "    MLPClassifier(solver='lbfgs',alpha=1e-4,hidden_layer_sizes=(10,10),random_state=1,verbose=True),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "]\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)\n",
    "parameters_clf = [{'n_neighbors':range(3,10)},\n",
    "              {'kernel':['rbf'], 'C':[0.01,0.1,1,10,100,1000]},\n",
    "              {'solver':['lbfgs'], 'alpha':[1e-5,1e-2], 'hidden_layer_sizes':[(10,10),(50,50),(100,100)]},\n",
    "              {'max_depth':[4,7,10,20],'n_estimators':[5,10,20],'max_features':[20,35,50]},\n",
    "              {'solver':['lbfgs'], 'alpha':[1e-5], 'hidden_layer_sizes':[(len(names)-1,len(names)-1),(len(names)-1,2)]}\n",
    "             ]\n",
    "makepipe_parameters_clf = [{'classifier__'+key:p[key] for key in p} for p in parameters_clf]\n",
    "makepipe_parameters_clf += [{'feature_selection__k': (750,500,100), 'feature_selection__score_func': [mutual_info_classif]},\n",
    "                            {'decomp__n_components': (100,50)}]\n",
    "metric = ['accuracy','f1']\n",
    "dataset = 0 # all datasets (0), dataset 1-2 (1), dataset 3 (2), dataset4 (3)\n",
    "download = 1 # Download pre-computed (1) data or compute them anew (0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_pipe_clf(scaler,feature_selection,decomp,clf):\n",
    "    order = 1\n",
    "# order = 1 : first perform feature selection and then apply PCA\n",
    "# order = 0 : first apply PCA and then reduce the transformed features\n",
    "    if order:\n",
    "        pipeline = Pipeline([('scaler', scaler),\n",
    "                    ('feature_selection', feature_selection),\n",
    "                    ('decomp', decomp),         \n",
    "                    ('classifier', clf) ])\n",
    "#     else:\n",
    "#         pipeline = Pipeline([('scaler', scaler),\n",
    "#                     ('decomp', decomp ),                 \n",
    "#                     ('feature_selection', feature_selection),        \n",
    "#                     ('classifier', clf) ])\n",
    "    return pipeline\n",
    "###########################################################################################\n",
    "def make_pipe(scaler,feature_selection,decomp,order):\n",
    "    if order:\n",
    "        pipeline = Pipeline([('scaler', scaler),\n",
    "                    ('feature_selection', feature_selection),\n",
    "                    ('decomp', decomp),         \n",
    "                     ])\n",
    "    else:\n",
    "        pipeline = Pipeline([('scaler', scaler),\n",
    "                    ('decomp', decomp ),                 \n",
    "                    ('feature_selection', feature_selection),        \n",
    "                     ])\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Nested_cv:\n",
    "    def __init__(self, n_outer_folds = 3, n_inner_folds = 3, n_top =1, state = 42):\n",
    "        self.n_outer_folds = n_outer_folds\n",
    "        self.n_inner_folds = n_inner_folds\n",
    "        self.n_top = n_top\n",
    "        self.state = state\n",
    "#         print(\"constructor %d\" %self.state)\n",
    "                                        \n",
    "    def set_pipe(self, pip_steps, pip_params):\n",
    "        n_steps = len(pip_steps)\n",
    "        if n_steps == 3:\n",
    "            self.pipe = make_pipe(scaler = pip_steps[0], feature_selection = pip_steps[1], decomp = pip_steps[2])\n",
    "            self.params = pip_params\n",
    "        if n_steps == 4:\n",
    "            self.pipe = make_pipe_clf(scaler = pip_steps[0], feature_selection = pip_steps[1], decomp = pip_steps[2], clf = pip_steps[3])\n",
    "            self.params = pip_params\n",
    "        else:\n",
    "            print (\"Number of steps gotta be either 3 or 4, you inserted %d.\" %n_steps)\n",
    "        return self.pipe\n",
    "        \n",
    "    def fit(self, x_tot, y_tot, verbose = 0):\n",
    "        self.verbose = verbose\n",
    "#         print(\"fit %d\" %self.state)\n",
    "        if self.n_top >1 :\n",
    "            print (\"ACHTUNG! You need to use fit2 for multiple best models!\")\n",
    "            self.fit2(x_tot,y_tot)\n",
    "        else:\n",
    "            self.outer_cv = StratifiedKFold(n_splits = self.n_outer_folds, shuffle = True, random_state = self.state)\n",
    "            self.inner_cv = StratifiedKFold(n_splits = self.n_inner_folds, shuffle = True, random_state = self.state)\n",
    "            self.grid = GridSearchCV(self.pipe, self.params, cv = self.inner_cv, verbose = self.verbose)       \n",
    "            self.score = cross_val_score(self.grid, X = x_tot, y = y_tot, cv = self.outer_cv, verbose = self.verbose)\n",
    "\n",
    "    def get_outer_scores(self):\n",
    "        return self.score\n",
    "    \n",
    "    def get_mean_score(self):\n",
    "        self.mean_score = self.score.mean()\n",
    "        print (\"Mean score of %d outer folds : %f\" %(self.n_outer_folds, self.mean_score ))\n",
    "        return self.mean_score\n",
    "    \n",
    "#     def combinations(self):\n",
    "#         self.comb = 1\n",
    "#         for self.key in self.params.keys():\n",
    "#             self.comb =self.comb*len(self.params[self.key])\n",
    "#         return self.comb\n",
    "    \n",
    "    def get_feat_scores(self):\n",
    "        return self.total_feats\n",
    "    \n",
    "    def get_best_features(self):\n",
    "        print(self.best_feat_ind)\n",
    "        return self.best_feat_ind\n",
    "    \n",
    "    def print_feat_scores(self):\n",
    "        self.norm_total_feats = (self.total_feats)/(self.n_inner_folds * self.n_top)\n",
    "        plt.figure()\n",
    "        self.rel_score = plt.bar(range(len(self.total_feats)),self.norm_total_feats)\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Score\")\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "    def repeated_cv(self,x_tot, y_tot, num_trials):\n",
    "        self.repeated = np.zeros(num_trials)\n",
    "        self.rep_feat_scores = np.zeros(x_tot.shape[1])\n",
    "        self.num_trials = num_trials\n",
    "        self.state +=1\n",
    "        for st in range(self.num_trials):\n",
    "            self.state +=1\n",
    "            self.fit(x_tot, y_tot)\n",
    "#             print(\"repeated %d\" %self.state)\n",
    "            self.out = self.get_outer_scores()\n",
    "            self.rep_feat_scores += self.get_feat_scores()\n",
    "            self.repeated[st] = self.out.mean()\n",
    "            \n",
    "        plt.figure()\n",
    "        self.rep_sc_plot = plt.bar(range(num_trials),self.repeated)\n",
    "        plt.xlabel(\"Individual trial #\")\n",
    "        plt.ylabel(\"Outer mean scores\")\n",
    "        plt.show()\n",
    "    \n",
    "    def print_rep_feat_scores(self):\n",
    "        plt.figure()\n",
    "        self.norm_rep_feats = (self.rep_feat_scores)/(self.n_inner_folds*self.n_top*self.num_trials)\n",
    "        self.rel_score = plt.bar(range(len(self.total_feats)),self.norm_rep_feats)\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"Feature Score\")\n",
    "        plt.show()\n",
    "        \n",
    "    def get_rep_feat_scores(self):\n",
    "        return self.rep_feat_scores\n",
    "        \n",
    "    def fit2(self,x_tot, y_tot):\n",
    "        #the \"handwritten\" implementation\n",
    "        self.total_feats = np.zeros(x_tot.shape[1])\n",
    "        self.classifiers= list(ParameterGrid(self.params))\n",
    "        self.outer_scores = np.zeros((self.n_top, self.n_outer_folds))\n",
    "#         self.ncomp = self.params['decomp__n_components']\n",
    "#         self.best_feat_ind = []\n",
    "        self.best_pipes = [[],[]]\n",
    "        self.outer_cv = StratifiedKFold(n_splits = self.n_outer_folds, shuffle = True, random_state = self.state)\n",
    "        self.inner_cv = StratifiedKFold(n_splits = self.n_inner_folds, shuffle = True, random_state = self.state)\n",
    "        self.outer = self.outer_cv.split(x_tot,y_tot)\n",
    "        for self.fold_out, (self.train_ind_out,self.test_ind_out) in enumerate(self.outer):\n",
    "            top_models = []\n",
    "            if self.verbose>0 :\n",
    "                print(\"Outer loop %d/%d\" %((self.fold_out + 1), self.n_outer_folds))\n",
    "        # split the dataset \n",
    "            self.x_trn_out, self.x_tst_out = x_tot[self.train_ind_out], x_tot[self.test_ind_out]\n",
    "            self.y_trn_out, self.y_tst_out = y_tot[self.train_ind_out], y_tot[self.test_ind_out]  \n",
    "            \n",
    "            self.inner_scores = np.zeros((len(self.classifiers),self.n_inner_folds))\n",
    "            self.inner_mean_scores = np.zeros(len(self.classifiers))\n",
    "            \n",
    "            for self.cl_ind, self.clf in enumerate(self.classifiers):\n",
    "                self.inner = self.inner_cv.split(self.x_trn_out, self.y_trn_out)\n",
    "                self.nfeat = self.classifiers[self.cl_ind]['feature_selection__k']\n",
    "                self.best_feat_ind = np.zeros((self.n_top, self.nfeat))\n",
    "\n",
    "\n",
    "                for self.fold_in, (self.train_ind_in, self.test_ind_in) in enumerate(self.inner): \n",
    "                    if self.verbose>0 :\n",
    "                        print(\"Inner fold %d/%d\" % ((self.fold_in + 1), self.n_inner_folds ))\n",
    "                    # split the datasets for the inner cv \n",
    "                    self.x_trn_in, self.x_tst_in = self.x_trn_out[self.train_ind_in], self.x_trn_out[self.test_ind_in]\n",
    "                    self.y_trn_in, self.y_tst_in = self.y_trn_out[self.train_ind_in], self.y_trn_out[self.test_ind_in]\n",
    "                    self.pip = self.pipe.set_params(**self.clf)\n",
    "                    self.pip.fit(self.x_trn_in,self.y_trn_in)\n",
    "                    self.inner_scores[self.cl_ind][self.fold_in] = self.pip.score(self.x_tst_in, self.y_tst_in)\n",
    "\n",
    "            # get the mean performance for every classifier\n",
    "            self.inner_mean_scores = np.mean(self.inner_scores, axis = 1)\n",
    "#             print('\\n',self.inner_mean_scores)\n",
    "\n",
    "            # sort the scores, low -> high \n",
    "            self.sorted_inds = self.inner_mean_scores.argsort()\n",
    "            self.sorted_scores = self.inner_mean_scores[self.sorted_inds]\n",
    "\n",
    "            print (\"Best %d models for outer fold %d are: \\n \" %(self.n_top, (self.fold_out+1)))\n",
    "\n",
    "#             get the inds of best performing models\n",
    "            self.temp2 = 0 #iterates over n_top models\n",
    "            for self.temp_ind in range(-1, -(self.n_top +1), -1):\n",
    "                self.actual_ind = self.sorted_inds[self.temp_ind]\n",
    "                self.best_pipes[1].append(self.classifiers[self.actual_ind]) \n",
    "                self.best_pipes[0].append(self.fold_out)\n",
    "                print(\"model no.%d \"%(self.actual_ind + 1))\n",
    "                print(self.classifiers[self.actual_ind])\n",
    "                #print the top features selected\n",
    "                self.best_fs = self.pipe.set_params(**self.classifiers[self.actual_ind]).named_steps['feature_selection']\n",
    "#                 best_fs = pipeline[actual_ind].named_steps['feature_selection']\n",
    "                self.best_fs_indd = self.best_fs.get_support(indices = True)\n",
    "#                 print(self.best_fs_indd)\n",
    "                self.pipe_fs_unsorted = self.best_fs.scores_\n",
    "                self.fs_inds = self.pipe_fs_unsorted.argsort()\n",
    "#                 print(self.fs_inds)\n",
    "                self.pipe_fs = self.pipe_fs_unsorted[self.fs_inds]\n",
    "                self.temp3 = 0 # temp3 = 0:number of features\n",
    "               \n",
    "                for self.temp_ind2 in range(-1, -(self.nfeat +1), -1):         \n",
    "                    self.best_feat_ind[self.temp2][self.temp3]=self.fs_inds[self.temp_ind2]\n",
    "                    self.total_feats[self.fs_inds[self.temp_ind2]]+=1 # feature scores \n",
    "                    self.temp3+=1\n",
    "#                 print(self.total_feats)\n",
    "                #fit the best classifier on the outer test data\n",
    "                self.best_pip = self.pipe.set_params(**self.classifiers[self.actual_ind])\n",
    "                \n",
    "                self.best_pip.fit(self.x_trn_out, self.y_trn_out)\n",
    "                \n",
    "                #get the outer score \n",
    "                self.outer_scores[self.temp2, self.fold_out] = self.best_pip.score(self.x_tst_out, self.y_tst_out)\n",
    "                print(\"Inner score: %f VS Outer score: %f \\n\" %(self.inner_mean_scores[self.actual_ind], self.outer_scores[self.temp2, self.fold_out]))\n",
    "                print (\"----------------------------------------------------------\")\n",
    "                self.temp2+=1\n",
    "\n",
    "\n",
    "#             print(\"Features selected: \\n\")\n",
    "        #     print(best_feat_ind.sort())     \n",
    "#             print(self.best_feat_ind)\n",
    "        # print(outer_scores)  \n",
    "        # print(best_pipes)\n",
    "        self.score = self.outer_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datasets = np.load('newfeatures_newdata_NOsample_1024_20_10_10000_XYsplit.npz')\n",
    "X = datasets['Xsp']\n",
    "Y = datasets['Ysp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8902, 3107), (8902, 3107), (8902, 6214), (8902, 3107), (8902, 3107), (8902, 6214), (8898, 3107), (8898, 3107), (8898, 6214)]\n"
     ]
    }
   ],
   "source": [
    "print([ X[i][j].shape for i in range(len(X)) for j in range(len(X[i]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.855699, total= 3.2min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.2min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.888827, total= 3.5min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  6.7min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.891011, total= 3.7min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.4min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.851124, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.0min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.796629, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min finished\n",
      "=============================================================\n",
      "Object : 0, classifier : KNei, 5-fold avg score = 0.856658\n",
      "[ 0.85569905  0.8888265   0.89101124  0.8511236   0.79662921]\n",
      "=============================================================\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.903425, total= 3.7min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.7min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.872544, total= 3.5min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.2min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.949438, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.8min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.929775, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.2min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.821910, total= 3.4min\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed: 17.6min finished\n",
      "=============================================================\n",
      "Object : 0, classifier : SVC(, 5-fold avg score = 0.895418\n",
      "[ 0.90342504  0.87254351  0.9494382   0.92977528  0.82191011]\n",
      "=============================================================\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.923077, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.6min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.957889, total= 3.7min\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  7.4min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.960674, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed: 10.9min remaining:    0.0s\n",
      "[CV]  ................................................................\n",
      "[CV] ................................. , score=0.927528, total= 3.6min\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed: 14.5min remaining:    0.0s\n",
      "[CV]  ................................................................\n"
     ]
    }
   ],
   "source": [
    "# i: iterating over material, j: over featureset \n",
    "# (consider keeping only the second featureset and discarding the rest to save some space)\n",
    "# Performing cross validation for each object on the second feature set \n",
    "\n",
    "cv = KFold(n_splits=5,random_state=42)\n",
    "# set the pipeline\n",
    "# def make_pipe_clf(scaler,feature_selection,decomp,clf):\n",
    "scaler = StandardScaler() ; featsel = SelectKBest(k=1000,score_func= mutual_info_classif)\n",
    "decomp = PCA(n_components=20)\n",
    "pipe_list = [make_pipe_clf(scaler,featsel, decomp, classifiers[i]) for i in range(len(classifiers))]\n",
    "scores = []\n",
    "for i in range(len(X)):\n",
    "    data = deepcopy(X[i][0])\n",
    "    print(i)\n",
    "    labels = deepcopy(Y[i])\n",
    "    for pipe in pipe_list:\n",
    "        score = cross_val_score(estimator = pipe, X = data, y = labels, cv = cv, verbose = 100)\n",
    "        scores.append(score)\n",
    "        print (\"=============================================================\")\n",
    "        print(\"Object : %d, classifier : %0.4s, 5-fold avg score = %f\" %(i, pipe.named_steps['classifier'], np.mean(score)))\n",
    "        print(score)\n",
    "        print (\"=============================================================\")\n",
    "        \n",
    "# print pipe_list[1].named_steps['classifier']\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
